{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SINTA Web Scraping - ETL dengan Hadoop\n",
        "\n",
        "Notebook ini akan:\n",
        "1. Clone repository dari GitHub\n",
        "2. Install dependencies\n",
        "3. Install dan setup Hadoop\n",
        "4. Menjalankan proses ETL\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Clone Repository\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository\n",
        "!git clone https://github.com/Herutriana44/sinta_web_scraping.git\n",
        "\n",
        "# Pindah ke direktori project\n",
        "import os\n",
        "os.chdir('sinta_web_scraping')\n",
        "\n",
        "print(\"‚úÖ Repository berhasil di-clone\")\n",
        "print(f\"üìÅ Current directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install Dependencies Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies dari requirements.txt\n",
        "!pip install -q selenium webdriver-manager bs4 beautifulsoup4 hdfs3\n",
        "\n",
        "print(\"‚úÖ Dependencies Python berhasil diinstall\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Install Java (Required untuk Hadoop)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Java JDK 8 (required untuk Hadoop)\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq openjdk-8-jdk > /dev/null 2>&1\n",
        "\n",
        "# Set JAVA_HOME\n",
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['PATH'] = os.environ['JAVA_HOME'] + '/bin:' + os.environ['PATH']\n",
        "\n",
        "# Verify Java installation\n",
        "!java -version\n",
        "\n",
        "print(\"\\n‚úÖ Java berhasil diinstall\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Install dan Setup Hadoop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Hadoop 3.3.6\n",
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "\n",
        "# Extract Hadoop\n",
        "!tar -xzf hadoop-3.3.6.tar.gz\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ['HADOOP_HOME'] = os.path.join(os.getcwd(), 'hadoop-3.3.6')\n",
        "os.environ['HADOOP_CONF_DIR'] = os.path.join(os.environ['HADOOP_HOME'], 'etc/hadoop')\n",
        "os.environ['PATH'] = os.path.join(os.environ['HADOOP_HOME'], 'bin') + ':' + \\\n",
        "                     os.path.join(os.environ['HADOOP_HOME'], 'sbin') + ':' + os.environ['PATH']\n",
        "\n",
        "print(\"‚úÖ Hadoop berhasil di-download dan di-extract\")\n",
        "print(f\"üìÅ HADOOP_HOME: {os.environ['HADOOP_HOME']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Konfigurasi Hadoop (Pseudo-Distributed Mode)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Path ke direktori konfigurasi Hadoop\n",
        "hadoop_conf_dir = os.path.join(os.environ['HADOOP_HOME'], 'etc/hadoop')\n",
        "\n",
        "# 1. Konfigurasi core-site.xml\n",
        "core_site_xml = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>fs.defaultFS</name>\n",
        "        <value>hdfs://localhost:9000</value>\n",
        "    </property>\n",
        "</configuration>'''\n",
        "\n",
        "with open(os.path.join(hadoop_conf_dir, 'core-site.xml'), 'w') as f:\n",
        "    f.write(core_site_xml)\n",
        "\n",
        "# 2. Konfigurasi hdfs-site.xml\n",
        "hdfs_site_xml = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>dfs.replication</name>\n",
        "        <value>1</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>dfs.namenode.name.dir</name>\n",
        "        <value>file:///tmp/hadoop-${user.name}/dfs/name</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>dfs.datanode.data.dir</name>\n",
        "        <value>file:///tmp/hadoop-${user.name}/dfs/data</value>\n",
        "    </property>\n",
        "</configuration>'''\n",
        "\n",
        "with open(os.path.join(hadoop_conf_dir, 'hdfs-site.xml'), 'w') as f:\n",
        "    f.write(hdfs_site_xml)\n",
        "\n",
        "# 3. Konfigurasi mapred-site.xml\n",
        "mapred_site_xml = '''<?xml version=\"1.0\"?>\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>mapreduce.framework.name</name>\n",
        "        <value>yarn</value>\n",
        "    </property>\n",
        "</configuration>'''\n",
        "\n",
        "with open(os.path.join(hadoop_conf_dir, 'mapred-site.xml'), 'w') as f:\n",
        "    f.write(mapred_site_xml)\n",
        "\n",
        "# 4. Konfigurasi yarn-site.xml\n",
        "yarn_site_xml = '''<?xml version=\"1.0\"?>\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>yarn.nodemanager.aux-services</name>\n",
        "        <value>mapreduce_shuffle</value>\n",
        "    </property>\n",
        "</configuration>'''\n",
        "\n",
        "with open(os.path.join(hadoop_conf_dir, 'yarn-site.xml'), 'w') as f:\n",
        "    f.write(yarn_site_xml)\n",
        "\n",
        "# 5. Set JAVA_HOME di hadoop-env.sh\n",
        "hadoop_env_sh = f'''export JAVA_HOME={os.environ['JAVA_HOME']}\n",
        "export HADOOP_HOME={os.environ['HADOOP_HOME']}\n",
        "export HADOOP_CONF_DIR={os.environ['HADOOP_CONF_DIR']}\n",
        "'''\n",
        "\n",
        "with open(os.path.join(hadoop_conf_dir, 'hadoop-env.sh'), 'a') as f:\n",
        "    f.write(hadoop_env_sh)\n",
        "\n",
        "print(\"‚úÖ Konfigurasi Hadoop selesai\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Setup SSH (Required untuk Hadoop)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install SSH server\n",
        "!apt-get install -y -qq openssh-server > /dev/null 2>&1\n",
        "\n",
        "# Setup SSH untuk Hadoop\n",
        "!ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa > /dev/null 2>&1\n",
        "!cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
        "!chmod 0600 ~/.ssh/authorized_keys\n",
        "\n",
        "# Start SSH service\n",
        "!service ssh start > /dev/null 2>&1\n",
        "\n",
        "# Test SSH connection\n",
        "!ssh-keyscan -H localhost >> ~/.ssh/known_hosts 2>/dev/null\n",
        "\n",
        "print(\"‚úÖ SSH berhasil dikonfigurasi\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Format HDFS dan Start Hadoop Services\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Format HDFS (hanya perlu dilakukan sekali, tapi aman untuk diulang)\n",
        "print(\"üìù Formatting HDFS...\")\n",
        "format_result = subprocess.run(\n",
        "    ['hdfs', 'namenode', '-format', '-force', '-nonInteractive'],\n",
        "    env=os.environ,\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "if format_result.returncode == 0:\n",
        "    print(\"‚úÖ HDFS berhasil di-format\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Format HDFS: {format_result.stderr}\")\n",
        "\n",
        "# Start NameNode\n",
        "print(\"\\nüöÄ Starting NameNode...\")\n",
        "namenode_process = subprocess.Popen(\n",
        "    ['hdfs', '--daemon', 'start', 'namenode'],\n",
        "    env=os.environ\n",
        ")\n",
        "time.sleep(5)\n",
        "\n",
        "# Start DataNode\n",
        "print(\"üöÄ Starting DataNode...\")\n",
        "datanode_process = subprocess.Popen(\n",
        "    ['hdfs', '--daemon', 'start', 'datanode'],\n",
        "    env=os.environ\n",
        ")\n",
        "time.sleep(5)\n",
        "\n",
        "# Check status\n",
        "print(\"\\nüìä Checking Hadoop services status...\")\n",
        "!jps\n",
        "\n",
        "print(\"\\n‚úÖ Hadoop services sudah berjalan\")\n",
        "print(\"üåê NameNode Web UI: http://localhost:9870\")\n",
        "print(\"üìÅ HDFS Path: hdfs://localhost:9000\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Verifikasi HDFS dan Buat Direktori\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Tunggu beberapa detik untuk memastikan HDFS siap\n",
        "import time\n",
        "time.sleep(3)\n",
        "\n",
        "# Test HDFS dengan membuat direktori\n",
        "print(\"üìÅ Membuat direktori di HDFS...\")\n",
        "\n",
        "# Buat direktori untuk data SINTA\n",
        "result = subprocess.run(\n",
        "    ['hdfs', 'dfs', '-mkdir', '-p', '/user/sinta/journals'],\n",
        "    env=os.environ,\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úÖ Direktori HDFS berhasil dibuat: /user/sinta/journals\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è {result.stderr}\")\n",
        "\n",
        "# List direktori HDFS\n",
        "print(\"\\nüìÇ Isi direktori HDFS:\")\n",
        "!hdfs dfs -ls -R /user/sinta/\n",
        "\n",
        "print(\"\\n‚úÖ HDFS siap digunakan!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Jalankan ETL Process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import ETL module\n",
        "from sinta_journals_etl import SINTAJournalsETL\n",
        "\n",
        "# Inisialisasi ETL dengan HDFS\n",
        "print(\"üîß Menginisialisasi ETL dengan HDFS...\")\n",
        "etl = SINTAJournalsETL(\n",
        "    input_folder=\"output_journals\",\n",
        "    output_folder=\"output_data\",\n",
        "    hdfs_enabled=True,\n",
        "    hdfs_url=\"http://localhost:9870\",\n",
        "    hdfs_path=\"/user/sinta/journals\",\n",
        "    hdfs_user=None  # Gunakan user default\n",
        ")\n",
        "\n",
        "# Jalankan ETL process\n",
        "print(\"\\nüöÄ Menjalankan proses ETL...\")\n",
        "etl.run(output_format='both', save_to_hdfs=True)\n",
        "\n",
        "print(\"\\n‚úÖ ETL Process selesai!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Verifikasi Hasil ETL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Cek file output lokal\n",
        "output_dir = Path('output_data')\n",
        "if output_dir.exists():\n",
        "    print(\"üìÅ File output lokal:\")\n",
        "    for file in sorted(output_dir.glob('*')):\n",
        "        size = file.stat().st_size / 1024  # KB\n",
        "        print(f\"  - {file.name} ({size:.2f} KB)\")\n",
        "    \n",
        "    # Tampilkan statistik jika ada\n",
        "    stats_files = list(output_dir.glob('extraction_stats_*.json'))\n",
        "    if stats_files:\n",
        "        latest_stats = sorted(stats_files)[-1]\n",
        "        with open(latest_stats, 'r') as f:\n",
        "            stats = json.load(f)\n",
        "        print(\"\\nüìä Statistik Ekstraksi:\")\n",
        "        print(json.dumps(stats['statistics'], indent=2))\n",
        "\n",
        "# Cek file di HDFS\n",
        "print(\"\\nüìÅ File di HDFS:\")\n",
        "!hdfs dfs -ls -R /user/sinta/journals/\n",
        "\n",
        "print(\"\\n‚úÖ Verifikasi selesai!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Stop Hadoop Services\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop Hadoop services jika diperlukan\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "print(\"üõë Menghentikan Hadoop services...\")\n",
        "\n",
        "subprocess.run(['hdfs', '--daemon', 'stop', 'datanode'], env=os.environ)\n",
        "subprocess.run(['hdfs', '--daemon', 'stop', 'namenode'], env=os.environ)\n",
        "\n",
        "print(\"‚úÖ Hadoop services dihentikan\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternatif: ETL Tanpa HDFS (Jika Hadoop Bermasalah)\n",
        "\n",
        "Jika mengalami masalah dengan Hadoop di Colab, jalankan ETL tanpa HDFS:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternatif: ETL tanpa HDFS jika Hadoop bermasalah\n",
        "from sinta_journals_etl import SINTAJournalsETL\n",
        "\n",
        "print(\"üîß Menginisialisasi ETL tanpa HDFS...\")\n",
        "etl_no_hdfs = SINTAJournalsETL(\n",
        "    input_folder=\"output_journals\",\n",
        "    output_folder=\"output_data\",\n",
        "    hdfs_enabled=False  # Nonaktifkan HDFS\n",
        ")\n",
        "\n",
        "# Jalankan ETL process\n",
        "print(\"\\nüöÄ Menjalankan proses ETL...\")\n",
        "etl_no_hdfs.run(output_format='both', save_to_hdfs=False)\n",
        "\n",
        "print(\"\\n‚úÖ ETL Process selesai (tanpa HDFS)!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
