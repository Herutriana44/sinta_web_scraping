{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SINTA Web Scraping - ETL dengan Hadoop\n",
        "\n",
        "Notebook ini akan:\n",
        "1. Clone repository dari GitHub\n",
        "2. Install dependencies\n",
        "3. Install dan setup Hadoop\n",
        "4. Menjalankan proses ETL\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Clone Repository\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository\n",
        "!git clone https://github.com/Herutriana44/sinta_web_scraping.git\n",
        "\n",
        "# Pindah ke direktori project\n",
        "import os\n",
        "os.chdir('sinta_web_scraping')\n",
        "\n",
        "print(\"‚úÖ Repository berhasil di-clone\")\n",
        "print(f\"üìÅ Current directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install Dependencies Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install build dependencies terlebih dahulu (untuk hdfs3)\n",
        "print(\"üì¶ Menginstall build dependencies...\")\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq build-essential libssl-dev libffi-dev python3-dev > /dev/null 2>&1\n",
        "\n",
        "# Install dependencies Python dasar\n",
        "print(\"üì¶ Menginstall dependencies Python dasar...\")\n",
        "!pip install -q selenium webdriver-manager bs4 beautifulsoup4\n",
        "\n",
        "# Coba install hdfs3 dengan build dependencies\n",
        "print(\"\\nüì¶ Mencoba install hdfs3...\")\n",
        "import subprocess\n",
        "result = subprocess.run(\n",
        "    ['pip', 'install', '-q', 'hdfs3'],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "# Cek apakah berhasil\n",
        "try:\n",
        "    import hdfs3\n",
        "    from hdfs3 import HDFileSystem\n",
        "    print(\"‚úÖ hdfs3 berhasil diinstall dan dapat diimport\")\n",
        "    HDFS3_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è hdfs3 gagal diinstall atau tidak dapat diimport\")\n",
        "    if result.stderr:\n",
        "        print(f\"   Error: {result.stderr[:200]}...\")  # Tampilkan sebagian error\n",
        "    print(\"üìù Notebook akan menggunakan alternatif: subprocess wrapper untuk HDFS\")\n",
        "    HDFS3_AVAILABLE = False\n",
        "\n",
        "print(\"\\n‚úÖ Dependencies Python berhasil diinstall\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2.1: Setup Alternatif HDFS (Jika hdfs3 Gagal)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Jika hdfs3 tidak tersedia, kita akan menggunakan subprocess untuk HDFS commands\n",
        "# Modifikasi sementara file ETL untuk mendukung alternatif\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Cek apakah hdfs3 tersedia\n",
        "try:\n",
        "    from hdfs3 import HDFileSystem\n",
        "    HDFS3_AVAILABLE = True\n",
        "    print(\"‚úÖ hdfs3 tersedia, akan menggunakan library hdfs3\")\n",
        "except ImportError:\n",
        "    HDFS3_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è hdfs3 tidak tersedia, akan menggunakan subprocess untuk HDFS\")\n",
        "    \n",
        "    # Buat wrapper untuk HDFS menggunakan subprocess\n",
        "    # Ini akan digunakan jika hdfs3 tidak tersedia\n",
        "    print(\"üìù Membuat wrapper HDFS menggunakan subprocess...\")\n",
        "    \n",
        "    # File wrapper akan dibuat setelah Hadoop diinstall\n",
        "    print(\"‚úÖ Wrapper akan dibuat setelah Hadoop diinstall\")\n",
        "\n",
        "print(f\"\\nüìä Status: HDFS3_AVAILABLE = {HDFS3_AVAILABLE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Install Java (Required untuk Hadoop)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Java JDK 8 (required untuk Hadoop)\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq openjdk-8-jdk > /dev/null 2>&1\n",
        "\n",
        "# Set JAVA_HOME\n",
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['PATH'] = os.environ['JAVA_HOME'] + '/bin:' + os.environ['PATH']\n",
        "\n",
        "# Verify Java installation\n",
        "!java -version\n",
        "\n",
        "print(\"\\n‚úÖ Java berhasil diinstall\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Install dan Setup Hadoop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Hadoop 3.3.6\n",
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "\n",
        "# Extract Hadoop\n",
        "!tar -xzf hadoop-3.3.6.tar.gz\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ['HADOOP_HOME'] = os.path.join(os.getcwd(), 'hadoop-3.3.6')\n",
        "os.environ['HADOOP_CONF_DIR'] = os.path.join(os.environ['HADOOP_HOME'], 'etc/hadoop')\n",
        "os.environ['PATH'] = os.path.join(os.environ['HADOOP_HOME'], 'bin') + ':' + \\\n",
        "                     os.path.join(os.environ['HADOOP_HOME'], 'sbin') + ':' + os.environ['PATH']\n",
        "\n",
        "print(\"‚úÖ Hadoop berhasil di-download dan di-extract\")\n",
        "print(f\"üìÅ HADOOP_HOME: {os.environ['HADOOP_HOME']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Konfigurasi Hadoop (Pseudo-Distributed Mode)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Path ke direktori konfigurasi Hadoop\n",
        "hadoop_conf_dir = os.path.join(os.environ['HADOOP_HOME'], 'etc/hadoop')\n",
        "\n",
        "# 1. Konfigurasi core-site.xml\n",
        "core_site_xml = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>fs.defaultFS</name>\n",
        "        <value>hdfs://localhost:9000</value>\n",
        "    </property>\n",
        "</configuration>'''\n",
        "\n",
        "with open(os.path.join(hadoop_conf_dir, 'core-site.xml'), 'w') as f:\n",
        "    f.write(core_site_xml)\n",
        "\n",
        "# 2. Konfigurasi hdfs-site.xml\n",
        "hdfs_site_xml = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>dfs.replication</name>\n",
        "        <value>1</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>dfs.namenode.name.dir</name>\n",
        "        <value>file:///tmp/hadoop-${user.name}/dfs/name</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>dfs.datanode.data.dir</name>\n",
        "        <value>file:///tmp/hadoop-${user.name}/dfs/data</value>\n",
        "    </property>\n",
        "</configuration>'''\n",
        "\n",
        "with open(os.path.join(hadoop_conf_dir, 'hdfs-site.xml'), 'w') as f:\n",
        "    f.write(hdfs_site_xml)\n",
        "\n",
        "# 3. Konfigurasi mapred-site.xml\n",
        "mapred_site_xml = '''<?xml version=\"1.0\"?>\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>mapreduce.framework.name</name>\n",
        "        <value>yarn</value>\n",
        "    </property>\n",
        "</configuration>'''\n",
        "\n",
        "with open(os.path.join(hadoop_conf_dir, 'mapred-site.xml'), 'w') as f:\n",
        "    f.write(mapred_site_xml)\n",
        "\n",
        "# 4. Konfigurasi yarn-site.xml\n",
        "yarn_site_xml = '''<?xml version=\"1.0\"?>\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>yarn.nodemanager.aux-services</name>\n",
        "        <value>mapreduce_shuffle</value>\n",
        "    </property>\n",
        "</configuration>'''\n",
        "\n",
        "with open(os.path.join(hadoop_conf_dir, 'yarn-site.xml'), 'w') as f:\n",
        "    f.write(yarn_site_xml)\n",
        "\n",
        "# 5. Set JAVA_HOME di hadoop-env.sh\n",
        "hadoop_env_sh = f'''export JAVA_HOME={os.environ['JAVA_HOME']}\n",
        "export HADOOP_HOME={os.environ['HADOOP_HOME']}\n",
        "export HADOOP_CONF_DIR={os.environ['HADOOP_CONF_DIR']}\n",
        "'''\n",
        "\n",
        "with open(os.path.join(hadoop_conf_dir, 'hadoop-env.sh'), 'a') as f:\n",
        "    f.write(hadoop_env_sh)\n",
        "\n",
        "print(\"‚úÖ Konfigurasi Hadoop selesai\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Setup SSH (Required untuk Hadoop)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install SSH server\n",
        "!apt-get install -y -qq openssh-server > /dev/null 2>&1\n",
        "\n",
        "# Setup SSH untuk Hadoop\n",
        "!ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa > /dev/null 2>&1\n",
        "!cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
        "!chmod 0600 ~/.ssh/authorized_keys\n",
        "\n",
        "# Start SSH service\n",
        "!service ssh start > /dev/null 2>&1\n",
        "\n",
        "# Test SSH connection\n",
        "!ssh-keyscan -H localhost >> ~/.ssh/known_hosts 2>/dev/null\n",
        "\n",
        "print(\"‚úÖ SSH berhasil dikonfigurasi\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Format HDFS dan Start Hadoop Services\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Format HDFS (hanya perlu dilakukan sekali, tapi aman untuk diulang)\n",
        "print(\"üìù Formatting HDFS...\")\n",
        "format_result = subprocess.run(\n",
        "    ['hdfs', 'namenode', '-format', '-force', '-nonInteractive'],\n",
        "    env=os.environ,\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "if format_result.returncode == 0:\n",
        "    print(\"‚úÖ HDFS berhasil di-format\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Format HDFS: {format_result.stderr}\")\n",
        "\n",
        "# Start NameNode\n",
        "print(\"\\nüöÄ Starting NameNode...\")\n",
        "namenode_process = subprocess.Popen(\n",
        "    ['hdfs', '--daemon', 'start', 'namenode'],\n",
        "    env=os.environ\n",
        ")\n",
        "time.sleep(5)\n",
        "\n",
        "# Start DataNode\n",
        "print(\"üöÄ Starting DataNode...\")\n",
        "datanode_process = subprocess.Popen(\n",
        "    ['hdfs', '--daemon', 'start', 'datanode'],\n",
        "    env=os.environ\n",
        ")\n",
        "time.sleep(5)\n",
        "\n",
        "# Check status\n",
        "print(\"\\nüìä Checking Hadoop services status...\")\n",
        "!jps\n",
        "\n",
        "print(\"\\n‚úÖ Hadoop services sudah berjalan\")\n",
        "print(\"üåê NameNode Web UI: http://localhost:9870\")\n",
        "print(\"üìÅ HDFS Path: hdfs://localhost:9000\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Verifikasi HDFS dan Buat Direktori\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Tunggu beberapa detik untuk memastikan HDFS siap\n",
        "import time\n",
        "time.sleep(3)\n",
        "\n",
        "# Test HDFS dengan membuat direktori\n",
        "print(\"üìÅ Membuat direktori di HDFS...\")\n",
        "\n",
        "# Buat direktori untuk data SINTA\n",
        "result = subprocess.run(\n",
        "    ['hdfs', 'dfs', '-mkdir', '-p', '/user/sinta/journals'],\n",
        "    env=os.environ,\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úÖ Direktori HDFS berhasil dibuat: /user/sinta/journals\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è {result.stderr}\")\n",
        "\n",
        "# List direktori HDFS\n",
        "print(\"\\nüìÇ Isi direktori HDFS:\")\n",
        "!hdfs dfs -ls -R /user/sinta/\n",
        "\n",
        "print(\"\\n‚úÖ HDFS siap digunakan!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Jalankan ETL Process\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8.1: Buat HDFS Wrapper (Jika hdfs3 Tidak Tersedia)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Buat wrapper HDFS menggunakan subprocess jika hdfs3 tidak tersedia\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Cek apakah hdfs3 tersedia\n",
        "try:\n",
        "    from hdfs3 import HDFileSystem\n",
        "    HDFS3_AVAILABLE = True\n",
        "    print(\"‚úÖ hdfs3 tersedia, akan menggunakan library hdfs3\")\n",
        "except ImportError:\n",
        "    HDFS3_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è hdfs3 tidak tersedia, akan membuat wrapper menggunakan subprocess...\")\n",
        "    \n",
        "if not HDFS3_AVAILABLE:\n",
        "    # Buat file helper untuk HDFS operations\n",
        "    hdfs_helper_code = '''\"\"\"\n",
        "HDFS Helper menggunakan subprocess (alternatif jika hdfs3 tidak tersedia)\n",
        "\"\"\"\n",
        "import os\n",
        "import subprocess\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "class HDFileSystemSubprocess:\n",
        "    \"\"\"Wrapper untuk HDFS menggunakan subprocess\"\"\"\n",
        "    \n",
        "    def __init__(self, host='localhost', port=9000, user=None):\n",
        "        self.host = host\n",
        "        self.port = port\n",
        "        self.user = user\n",
        "        self.env = os.environ.copy()\n",
        "    \n",
        "    def exists(self, path):\n",
        "        \"\"\"Cek apakah path ada di HDFS\"\"\"\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                ['hdfs', 'dfs', '-test', '-e', path],\n",
        "                env=self.env,\n",
        "                capture_output=True,\n",
        "                text=True\n",
        "            )\n",
        "            return result.returncode == 0\n",
        "        except:\n",
        "            return False\n",
        "    \n",
        "    def makedirs(self, path):\n",
        "        \"\"\"Buat direktori di HDFS\"\"\"\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                ['hdfs', 'dfs', '-mkdir', '-p', path],\n",
        "                env=self.env,\n",
        "                capture_output=True,\n",
        "                text=True\n",
        "            )\n",
        "            return result.returncode == 0\n",
        "        except:\n",
        "            return False\n",
        "    \n",
        "    def open(self, path, mode='rb'):\n",
        "        \"\"\"Open file di HDFS (untuk kompatibilitas dengan hdfs3)\"\"\"\n",
        "        if 'w' in mode or 'a' in mode:\n",
        "            return HDFSFileWriter(path, self.env)\n",
        "        else:\n",
        "            return HDFSFileReader(path, self.env)\n",
        "\n",
        "class HDFSFileWriter:\n",
        "    \"\"\"File writer untuk HDFS menggunakan subprocess\"\"\"\n",
        "    def __init__(self, hdfs_path, env):\n",
        "        self.hdfs_path = hdfs_path\n",
        "        self.env = env\n",
        "        self.temp_file = tempfile.NamedTemporaryFile(delete=False)\n",
        "        self.temp_path = self.temp_file.name\n",
        "        self.closed = False\n",
        "    \n",
        "    def write(self, data):\n",
        "        if not self.closed:\n",
        "            self.temp_file.write(data)\n",
        "    \n",
        "    def close(self):\n",
        "        if not self.closed:\n",
        "            self.temp_file.close()\n",
        "            # Upload ke HDFS\n",
        "            subprocess.run(\n",
        "                ['hdfs', 'dfs', '-put', '-f', self.temp_path, self.hdfs_path],\n",
        "                env=self.env,\n",
        "                capture_output=True\n",
        "            )\n",
        "            # Hapus temp file\n",
        "            try:\n",
        "                os.unlink(self.temp_path)\n",
        "            except:\n",
        "                pass\n",
        "            self.closed = True\n",
        "    \n",
        "    def __enter__(self):\n",
        "        return self\n",
        "    \n",
        "    def __exit__(self, *args):\n",
        "        self.close()\n",
        "\n",
        "class HDFSFileReader:\n",
        "    \"\"\"File reader untuk HDFS menggunakan subprocess\"\"\"\n",
        "    def __init__(self, hdfs_path, env):\n",
        "        self.hdfs_path = hdfs_path\n",
        "        self.env = env\n",
        "        self.temp_file = tempfile.NamedTemporaryFile(delete=False)\n",
        "        self.temp_path = self.temp_file.name\n",
        "        # Download dari HDFS\n",
        "        subprocess.run(\n",
        "            ['hdfs', 'dfs', '-get', hdfs_path, self.temp_path],\n",
        "            env=self.env,\n",
        "            capture_output=True\n",
        "        )\n",
        "        self.file = open(self.temp_path, 'rb')\n",
        "        self.closed = False\n",
        "    \n",
        "    def read(self, size=-1):\n",
        "        return self.file.read(size)\n",
        "    \n",
        "    def close(self):\n",
        "        if not self.closed:\n",
        "            self.file.close()\n",
        "            try:\n",
        "                os.unlink(self.temp_path)\n",
        "            except:\n",
        "                pass\n",
        "            self.closed = True\n",
        "    \n",
        "    def __enter__(self):\n",
        "        return self\n",
        "    \n",
        "    def __exit__(self, *args):\n",
        "        self.close()\n",
        "'''\n",
        "    \n",
        "    # Simpan helper code\n",
        "    with open('hdfs_helper.py', 'w') as f:\n",
        "        f.write(hdfs_helper_code)\n",
        "    \n",
        "    # Modifikasi sinta_journals_etl.py untuk menggunakan helper jika hdfs3 tidak tersedia\n",
        "    print(\"üìù Memodifikasi sinta_journals_etl.py untuk menggunakan subprocess wrapper...\")\n",
        "    \n",
        "    # Baca file ETL\n",
        "    with open('sinta_journals_etl.py', 'r', encoding='utf-8') as f:\n",
        "        etl_code = f.read()\n",
        "    \n",
        "    # Ganti import hdfs3 dengan fallback ke helper\n",
        "    modified_import = '''# HDFS support (optional)\n",
        "try:\n",
        "    from hdfs3 import HDFileSystem\n",
        "    HDFS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    try:\n",
        "        # Coba gunakan helper subprocess\n",
        "        from hdfs_helper import HDFileSystemSubprocess as HDFileSystem\n",
        "        HDFS_AVAILABLE = True\n",
        "    except ImportError:\n",
        "        HDFS_AVAILABLE = False\n",
        "        logger.warning(\"‚ö†Ô∏è Library hdfs3 tidak tersedia. Fitur HDFS akan dinonaktifkan.\")\n",
        "        logger.warning(\"‚ö†Ô∏è Install dengan: pip install hdfs3\")\n",
        "'''\n",
        "    \n",
        "    # Ganti bagian import\n",
        "    old_import = '''# HDFS support (optional)\n",
        "try:\n",
        "    from hdfs3 import HDFileSystem\n",
        "    HDFS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    HDFS_AVAILABLE = False\n",
        "    logger.warning(\"‚ö†Ô∏è Library hdfs3 tidak tersedia. Fitur HDFS akan dinonaktifkan.\")\n",
        "    logger.warning(\"‚ö†Ô∏è Install dengan: pip install hdfs3\")'''\n",
        "    \n",
        "    if old_import in etl_code:\n",
        "        etl_code = etl_code.replace(old_import, modified_import)\n",
        "        \n",
        "        # Simpan file yang dimodifikasi\n",
        "        with open('sinta_journals_etl.py', 'w', encoding='utf-8') as f:\n",
        "            f.write(etl_code)\n",
        "        \n",
        "        print(\"‚úÖ File ETL dimodifikasi untuk menggunakan subprocess wrapper\")\n",
        "        print(\"‚úÖ HDFS wrapper menggunakan subprocess siap digunakan\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Tidak dapat menemukan bagian import hdfs3 untuk dimodifikasi\")\n",
        "        print(\"‚ö†Ô∏è Pastikan file sinta_journals_etl.py ada di direktori\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import ETL module\n",
        "from sinta_journals_etl import SINTAJournalsETL\n",
        "\n",
        "# Inisialisasi ETL dengan HDFS\n",
        "print(\"üîß Menginisialisasi ETL dengan HDFS...\")\n",
        "etl = SINTAJournalsETL(\n",
        "    input_folder=\"output_journals\",\n",
        "    output_folder=\"output_data\",\n",
        "    hdfs_enabled=True,\n",
        "    hdfs_url=\"http://localhost:9870\",\n",
        "    hdfs_path=\"/user/sinta/journals\",\n",
        "    hdfs_user=None  # Gunakan user default\n",
        ")\n",
        "\n",
        "# Jalankan ETL process\n",
        "print(\"\\nüöÄ Menjalankan proses ETL...\")\n",
        "etl.run(output_format='both', save_to_hdfs=True)\n",
        "\n",
        "print(\"\\n‚úÖ ETL Process selesai!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Verifikasi Hasil ETL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9.1: Cek Lokasi File di HDFS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cek lokasi file di HDFS setelah ETL selesai\n",
        "import subprocess\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"üìÅ Struktur Direktori HDFS:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Base Path: /user/sinta/journals\")\n",
        "print(\"Struktur: /user/sinta/journals/{YYYY}/{MM}/{DD}/{filename}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Dapatkan tanggal hari ini untuk melihat folder\n",
        "today = datetime.now().strftime(\"%Y/%m/%d\")\n",
        "hdfs_date_path = f\"/user/sinta/journals/{today}\"\n",
        "\n",
        "print(f\"\\nüìÇ Path lengkap untuk hari ini: {hdfs_date_path}\")\n",
        "print(\"\\nüîç Listing semua file di HDFS:\")\n",
        "\n",
        "# List semua file di direktori base\n",
        "result = subprocess.run(\n",
        "    ['hdfs', 'dfs', '-ls', '-R', '/user/sinta/journals/'],\n",
        "    env=os.environ,\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(result.stdout)\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Error: {result.stderr}\")\n",
        "    print(\"\\nüí° Coba list secara manual:\")\n",
        "    print(f\"   !hdfs dfs -ls -R /user/sinta/journals/\")\n",
        "\n",
        "print(\"\\nüìù Cara mengakses file di HDFS:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. Melalui command line:\")\n",
        "print(\"   !hdfs dfs -cat /user/sinta/journals/YYYY/MM/DD/journals_data_*.csv\")\n",
        "print(\"\\n2. Download dari HDFS ke lokal:\")\n",
        "print(\"   !hdfs dfs -get /user/sinta/journals/YYYY/MM/DD/journals_data_*.csv .\")\n",
        "print(\"\\n3. Copy dari HDFS ke lokal:\")\n",
        "print(\"   !hdfs dfs -copyToLocal /user/sinta/journals/YYYY/MM/DD/journals_data_*.csv .\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Cek file output lokal\n",
        "output_dir = Path('output_data')\n",
        "if output_dir.exists():\n",
        "    print(\"üìÅ File output lokal:\")\n",
        "    for file in sorted(output_dir.glob('*')):\n",
        "        size = file.stat().st_size / 1024  # KB\n",
        "        print(f\"  - {file.name} ({size:.2f} KB)\")\n",
        "    \n",
        "    # Tampilkan statistik jika ada\n",
        "    stats_files = list(output_dir.glob('extraction_stats_*.json'))\n",
        "    if stats_files:\n",
        "        latest_stats = sorted(stats_files)[-1]\n",
        "        with open(latest_stats, 'r') as f:\n",
        "            stats = json.load(f)\n",
        "        print(\"\\nüìä Statistik Ekstraksi:\")\n",
        "        print(json.dumps(stats['statistics'], indent=2))\n",
        "\n",
        "# Cek file di HDFS\n",
        "print(\"\\nüìÅ File di HDFS:\")\n",
        "!hdfs dfs -ls -R /user/sinta/journals/\n",
        "\n",
        "print(\"\\n‚úÖ Verifikasi selesai!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Stop Hadoop Services\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop Hadoop services jika diperlukan\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "print(\"üõë Menghentikan Hadoop services...\")\n",
        "\n",
        "subprocess.run(['hdfs', '--daemon', 'stop', 'datanode'], env=os.environ)\n",
        "subprocess.run(['hdfs', '--daemon', 'stop', 'namenode'], env=os.environ)\n",
        "\n",
        "print(\"‚úÖ Hadoop services dihentikan\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternatif: ETL Tanpa HDFS (Jika Hadoop Bermasalah)\n",
        "\n",
        "Jika mengalami masalah dengan Hadoop di Colab, jalankan ETL tanpa HDFS:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternatif: ETL tanpa HDFS jika Hadoop bermasalah\n",
        "from sinta_journals_etl import SINTAJournalsETL\n",
        "\n",
        "print(\"üîß Menginisialisasi ETL tanpa HDFS...\")\n",
        "etl_no_hdfs = SINTAJournalsETL(\n",
        "    input_folder=\"output_journals\",\n",
        "    output_folder=\"output_data\",\n",
        "    hdfs_enabled=False  # Nonaktifkan HDFS\n",
        ")\n",
        "\n",
        "# Jalankan ETL process\n",
        "print(\"\\nüöÄ Menjalankan proses ETL...\")\n",
        "etl_no_hdfs.run(output_format='both', save_to_hdfs=False)\n",
        "\n",
        "print(\"\\n‚úÖ ETL Process selesai (tanpa HDFS)!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
